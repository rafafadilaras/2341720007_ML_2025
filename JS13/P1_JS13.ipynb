{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Jobsheet 13 - Artificial Neural Network (ANN)**"
      ],
      "metadata": {
        "id": "DGmGH_DiN5Nc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Praktikum 1 - JST sederhana (2 layer) dengan forward pass dan backpropagation manual**"
      ],
      "metadata": {
        "id": "1ypBE3hLOGB9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRxXP-IqNILv",
        "outputId": "241f6168-5793-4f7a-e6ef-d997ecfc3e42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.26845216898128865\n",
            "Epoch 1000, Loss: 0.23743952738078528\n",
            "Epoch 2000, Loss: 0.18136189805229946\n",
            "Epoch 3000, Loss: 0.14793584924684955\n",
            "Epoch 4000, Loss: 0.13742788936747824\n",
            "Epoch 5000, Loss: 0.1331412007220042\n",
            "Epoch 6000, Loss: 0.13094205807660247\n",
            "Epoch 7000, Loss: 0.12963513273465638\n",
            "Epoch 8000, Loss: 0.1287792320890651\n",
            "Epoch 9000, Loss: 0.12817938561855696\n",
            "Prediksi:\n",
            "[[0.0418    ]\n",
            " [0.49833313]\n",
            " [0.94833704]\n",
            " [0.50484248]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Parameter\n",
        "input_size = 2\n",
        "hidden_size = 2\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi bobot\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Fungsi aktivasi\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Training\n",
        "for epoch in range(10000):\n",
        "    # Forward pass\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    # Hitung error\n",
        "    error = y - a2\n",
        "\n",
        "    # Backpropagation\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update bobot\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Output akhir\n",
        "print(\"Prediksi:\")\n",
        "print(a2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tugas 1**"
      ],
      "metadata": {
        "id": "FVUElYblRs3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Mengubah jumlah neuron hiden layer menjadi 3"
      ],
      "metadata": {
        "id": "meaNJTOlR4Zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Parameter\n",
        "input_size = 2\n",
        "hidden_size = 3\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi bobot\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Fungsi aktivasi\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Training\n",
        "for epoch in range(10000):\n",
        "    # Forward pass\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    # Hitung error\n",
        "    error = y - a2\n",
        "\n",
        "    # Backpropagation\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update bobot\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Output akhir\n",
        "print(\"Prediksi:\")\n",
        "print(a2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKqLNihNSA8H",
        "outputId": "634c49c1-747e-4aa6-fd72-0d892dae2fec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.3068925545283221\n",
            "Epoch 1000, Loss: 0.1940046599023882\n",
            "Epoch 2000, Loss: 0.13501645305005108\n",
            "Epoch 3000, Loss: 0.03944073939866964\n",
            "Epoch 4000, Loss: 0.014921874020480876\n",
            "Epoch 5000, Loss: 0.008219931925919046\n",
            "Epoch 6000, Loss: 0.00545620174537408\n",
            "Epoch 7000, Loss: 0.0040126499614300945\n",
            "Epoch 8000, Loss: 0.00314365034475404\n",
            "Epoch 9000, Loss: 0.002569605387275079\n",
            "Prediksi:\n",
            "[[0.02486432]\n",
            " [0.95593289]\n",
            " [0.95035328]\n",
            " [0.0603007 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Membandingkan hasil loss dengan konfigurasi awal."
      ],
      "metadata": {
        "id": "kAnum2RUrfJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berdasarkan hasil percobaan, dengan menambah jumlah neuron pada hidden layer dari 2 menjadi 3 mampu meningkatkan kemampuan belajar model. Pada konfigurasi awal (2 neuron), model gagal mempelajari pola XOR sepenuhnya dan terjebak dalam kondisi stuck, yang ditandai dengan nilai loss yang tertahan tinggi (0.128) dan prediksi angka yang ragu-ragu di sekitar 0.5. Sebaliknya, dengan menggunakan 3 neuron, jaringan memiliki kapasitas yang lebih besar dan fleksibel untuk memisahkan data. Hal ini membuat proses belajar menjadi lancar, menghasilkan loss yang turun drastis hingga mendekati nol (0.002), serta memberikan hasil prediksi yang sangat tegas dan akurat sesuai target."
      ],
      "metadata": {
        "id": "Y54TMSnYsQGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Menambahkan fungsi aktivasi ReLU dan bandingkan hasil"
      ],
      "metadata": {
        "id": "-a-Mv7G6tdLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Parameter (Tetap menggunakan 3 neuron hidden agar adil dengan percobaan sebelumnya)\n",
        "input_size = 2\n",
        "hidden_size = 3\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "np.random.seed(42) # Seed sama agar perbandingan fair\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# --- DEFINISI FUNGSI AKTIVASI BARU ---\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    # Turunan ReLU: 1 jika x > 0, selain itu 0\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "# Training\n",
        "for epoch in range(10000):\n",
        "    # --- FORWARD PASS ---\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = relu(z1)       # UBAH: Hidden layer sekarang pakai ReLU\n",
        "\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)    # Output layer tetap Sigmoid\n",
        "\n",
        "    # Hitung error\n",
        "    error = y - a2\n",
        "\n",
        "    # --- BACKPROPAGATION ---\n",
        "    # Layer Output (Tetap Sigmoid)\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    # Layer Hidden (Sekarang ReLU)\n",
        "    # Perhatikan: Turunan ReLU menggunakan z1 (input sebelum aktivasi)\n",
        "    d_a1 = np.dot(d_a2, W2.T) * relu_derivative(z1)\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update bobot\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
        "\n",
        "# Output akhir\n",
        "print(\"\\nPrediksi Akhir (ReLU di Hidden Layer):\")\n",
        "print(a2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bykr1I2Ftcpw",
        "outputId": "34c7c43a-91f9-4921-e814-8cbe6be31ae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.327478\n",
            "Epoch 1000, Loss: 0.012797\n",
            "Epoch 2000, Loss: 0.003283\n",
            "Epoch 3000, Loss: 0.001731\n",
            "Epoch 4000, Loss: 0.001147\n",
            "Epoch 5000, Loss: 0.000846\n",
            "Epoch 6000, Loss: 0.000666\n",
            "Epoch 7000, Loss: 0.000547\n",
            "Epoch 8000, Loss: 0.000462\n",
            "Epoch 9000, Loss: 0.000400\n",
            "\n",
            "Prediksi Akhir (ReLU di Hidden Layer):\n",
            "[[0.02970009]\n",
            " [0.98605912]\n",
            " [0.98605934]\n",
            " [0.01163533]]\n"
          ]
        }
      ]
    }
  ]
}